# -*- coding: utf-8 -*-
"""Untitled16.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JSpv_OzcSDCKHTdtRcboHauCLL7eghDW
"""

!pip install -q findspark
!pip install pyspark
### ipynb파일이었음


from pyspark.sql import SparkSession

from pyspark.sql.functions import when
# from pyspark.sql.types import
import datetime
import pandas as pd
import numpy as np

import findspark
findspark.init()


spark = SparkSession.builder.appName("press").master("local[*]").getOrCreate()# 스파크 세션 미리지정

df = pd.read_excel("Press_RawDataSet.xlsx")#독립변수 데이터셋

df_error = pd.read_excel("Press_error.xlsx",usecols="A:F" )

df_input = spark.read.csv("input_data.csv", header=True, inferSchema=True)

df['working time'].iloc[49423 : 52558] = '2020-05-26 00:00:00'# 26일 주소범위만큼의 값을 2020-05-26 00:00:00으로 채움

df['working time'] = pd.to_datetime(df['working time'], format='%Y-%m-%d %H:%M:%S', errors='coerce')#날짜 타입으로 변환

df['working time'].iloc[49423 : 52558].info()

df.isna().sum()

df.info()

df.describe()

df.drop(["idx", "Item No", "Machine_Name"], axis=1, inplace = True) # 필요없는 컬럼들 (지울때 inplace = True 추가)
df_error.drop(["idx", "Item No", "Machine_Name"], axis=1, inplace = True) # 필요없는 컬럼들 (지울때 inplace = True 추가)
# Join할때 쓰일수도 있어서 일단 남김

# prompt: df save

df.to_csv('processed_press_data.csv', index=False)
#df.to_excel('processed_press_data.xlsx', index=False)

"""## 우선 이상치 처리 데이터 저장함

"""

df_input.show()

df.info()

df.isnull().sum()

"""## 의문인 점 :
### 1. 종속변수와 독립변수 컬럼의 Join의 필요유무, 필요하다면 어떤방식을 쓸것인가
### 2. 어떤 독립변수값에서 불량(종속)이 났는지 모름, 또 현황분석에 적합한 데이터셋은 아닌것같음
###(6만번의 작동중에 몇%정도의 불량이 나는지 정도는 가능할듯)
#### #해결 : 비지도 학습이었음으로 조인,종속변수 필요 X(지금 전처리된 데이터를 스케일링 시키고 가우시안등 군집모델로 비지도 훈련시키면 됨)
"""

df_input.show()# Press time(ms), Pressure 1, Pressure 2, Pressure 5를 scale 시킨거 현재 전처리한 독립변수를 스케일링 하면 됨

"""## 이제 군집분석 모델 생성으로 들어갈듯"""

from sklearn.preprocessing import StandardScaler

data_X = df[["Press time(ms)", "Pressure 1", "Pressure 2", "Pressure 5"]]

std_scaler = StandardScaler()

scaled_X = std_scaler.fit_transform(data_X)

# prompt: numpy array view shape

scaled_X.shape

"""### 스케일링까지 완료"""









df_spark = spark.createDataFrame(df)# 스파크 데이터프레임으로 변경

df_spark.describe().show()

df_spark.printSchema()

spark_rdd = df_spark.rdd# RDD 형태로도 만들어 둠(언젠가 넘겨줄때 RDD로 넘겨야할듯)

spark_rdd.take(5)

